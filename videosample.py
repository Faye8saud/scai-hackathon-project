# -*- coding: utf-8 -*-
"""videoSample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wDBvpkwFjEghADsYNMX3-Oa4dpN5SXyy
"""

!pip install -q openai-whisper
!pip install -q ffmpeg-python

!pip install spacy
!python -m spacy download en_core_web_sm
!python -m spacy download ar_core_news_md

!pip install opencv-python

import cv2
import numpy as np
import whisper
import spacy
import os
import subprocess
from google.colab import files

# Step 1: Upload Video File
uploaded = files.upload()
video_filename = list(uploaded.keys())[0]

# Step 2: Transcribe Video Using Whisper
model = whisper.load_model("medium")
result = model.transcribe(video_filename)
transcription_text = result["text"]

# Save transcription
with open("transcription.txt", "w") as file:
    file.write(transcription_text)

print("Transcription Done!")

# Step 3: Process Text with spaCy
nlp = spacy.load("en_core_web_sm")
doc = nlp(transcription_text)

# Step 4: Define Icons and Search Terms
icons = {
    "ضربة جزاء": "warning.png",
    "ضربة الجزاء": "warning.png",
    "الضربه": "warning.png",
    "الجزاء ": "warning.png",
    "هات تريك": "warning.png",
    "ركلة جزاء": "warning.png",
    "لمسة يد": "warning.png",
    "ركنية": "warning.png",
    "الركنية": "warning.png",
    "ركنيه": "warning.png",
    "ريمنتادا": "warning.png",
    "ريمونتادا": "warning.png",
    "ريمون تادا ": "warning.png",
    "الريمون تادا": "warning.png",
    "تسلل": "warning.png",
    "لمسة": "warning.png",
    "لمسة يد ": "warning.png",
    "لمسه يد": "warning.png",
    "انذار": "warning.png",
    "الانذار": "warning.png",
    "صافرة": "warning.png",
    "المرتدة": "warning.png",
    "مرتدة": "warning.png",
    "مرتده": "warning.png",
    "المرتده": "warning.png",
    "التسلل": "warning.png",
}

# Ensure all words from icons are in search_terms
search_terms = list(icons.keys())

timestamps = []
for segment in result["segments"]:
    text = segment["text"]
    start_time = segment["start"]

    for term in search_terms:
        if term in text:  # Check if the word/phrase appears
            timestamps.append((term, start_time))

print("Words & Phrases Found:", timestamps)

# Step 5: Process Video with OpenCV
cap = cv2.VideoCapture(video_filename)
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Define Video Writer
temp_video = "temp_output.mp4"
out = cv2.VideoWriter(temp_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))

# Load & Resize Icons
for key in icons:
    icons[key] = cv2.imread(icons[key], cv2.IMREAD_UNCHANGED)
    icons[key] = cv2.resize(icons[key], (150, 150))

frame_number = 0
icon_display_time = 4  # Show icons for 4 seconds

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    current_time = frame_number / fps  # Convert frame count to seconds

    for term, timestamp in timestamps:
        if timestamp <= current_time < timestamp + icon_display_time:  # Display for 4 seconds
            if term in icons:
                icon = icons[term]
                x, y = frame_width - 170, frame_height - 170  # Bottom-right position

                h, w, _ = icon.shape
                for c in range(3):
                    frame[y:y+h, x:x+w, c] = (
                        icon[:, :, c] * (icon[:, :, 3] / 255.0) +
                        frame[y:y+h, x:x+w, c] * (1.0 - icon[:, :, 3] / 255.0)
                    )

    out.write(frame)
    frame_number += 1

cap.release()
out.release()
cv2.destroyAllWindows()

print("Processing Done! Now merging audio...")

# Step 6: Merge Original Audio with Processed Video Using FFmpeg
output_filename = "output_video.mp4"
subprocess.run([
    "ffmpeg", "-i", temp_video, "-i", video_filename, "-c:v", "copy", "-c:a", "aac", "-strict", "experimental", "-map", "0:v:0", "-map", "1:a:0", output_filename, "-y"
])

# Step 7: Provide Download Link for Output Video
files.download(output_filename)